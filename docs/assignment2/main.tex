\documentclass{article}
\usepackage{graphicx}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}
\usepackage{graphicx}
\usepackage{indentfirst}
\usepackage{tcolorbox}
\usepackage{xcolor}
\usepackage{amsmath}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\usepackage{listings}
\lstset{language=C++}
\lstset{breaklines=true}
\lstset{
    literate={"}{{\texttt{"}}}1
             {'}{{\texttt{'}}}1
             {~}{{\texttt{~}}}1
             { }{{\ }}1
}
\lstdefinestyle{code}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=t,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2,
    language=C++
}

\lstdefinestyle{output}{
    backgroundcolor=\color{black!5},
    basicstyle=\ttfamily\small,
    breaklines=true,
    showstringspaces=false,
    frame=single,
    rulecolor=\color{black!20},
    numbers=none,
    captionpos=t,
    language=bash
}

\renewcommand{\lstlistingname}{Code block}


\newcounter{exercise}
\newenvironment{exr}[1]{%
    \refstepcounter{exercise}
    \begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=Exercise \theexercise]
    \textbf{Instructions:} #1
    \end{tcolorbox}
    \vspace{1em}
}{}

\patchcmd{\thebibliography}{\section*{\refname}}{}{}{}

\title{
    TAC-HEP GPU Programming Training Module: Assignment 2
}

\author{Roy F. Cruz}

\begin{document}

\date{October 12, 2025}
\maketitle

The GitHub repository that hosts this code can be accessed via the following link: \href{https://github.com/roy-cruz/TAC-HEP_GPU-Course_Assignments/tree/master}{link}

\begin{exr}{
    \begin{itemize}
        \item Write a CUDA kernel that swaps the elements of two vectors.
        \item Start from file \texttt{swap\_vectors.cu}.
    \end{itemize}
    }
\end{exr}

The kernel in this implementation works by first using a temporary variable, \texttt{temp}, to store the element of \texttt{A} located at index \texttt{idx = threadIdx.x + blockDim.x * blockIdx.x}. It then replaces that element in \texttt{A} with the corresponding element from \texttt{B} at the same index. Finally, the value stored in \texttt{temp} is written back into \texttt{B} at that same position, effectively swapping the two elements. By ensuring that only threads with \texttt{idx < n\_elements} are executed, the kernel guarantees that no out-of-bounds memory access occurs.

\lstinputlisting[caption={\texttt{swap\_vectors.cu}}, style=code]{../../assignment2/swap_vectors.cu}

\begin{lstlisting}[style=output]
[rcruzcan@g37n01 assignment2]$ nvcc swap_vectors.cu -o objs/swap_vectors
[rcruzcan@g37n01 assignment2]$ ./objs/swap_vectors 
First 4 elements of the arrays before swapping
A = 0.840188, 0.783099, 0.911647, 0.335223, 
B = 0.394383, 0.798440, 0.197551, 0.768230, 

==============

First 4 elements of the arrays after swapping
A = 0.394383, 0.798440, 0.197551, 0.768230, 
B = 0.840188, 0.783099, 0.911647, 0.335223, 
\end{lstlisting}


% ------------------------------------------------------------------------


\begin{exr}{
    \begin{itemize}
      \item Write a CUDA kernel that adds two matrices of size $N\times M$.
      \item Express the indices of the matrices in 1 dimension:
        \begin{itemize}
          \item In a 1-d array the 2-d matrix will look like:
            \begin{align*}
            [A_{11},\, A_{12},\, A_{13},\, \dots,\, A_{1M}, A_{21},\, A_{22},\, \dots,\, A_{2M}, \dots, A_{N1},\, A_{N2},\, \dots,\, A_{NM}]
            \end{align*}
          \item For a matrix of size $N\times M$ each element can be expressed as \\
            \texttt{2D:[i,j] = 1D:[i * M + j]}
        \end{itemize}
      \item Use 2-dimensions to express threads and blocks: \texttt{threadIdx.x}, \texttt{blockIdx.x}, \texttt{threadIdx.y}, \texttt{blockIdx.y}.
      \item Start from file \texttt{add\_matrix.cu}.
    \end{itemize}   
}\end{exr}

The matrix addition kernel works by using each threadâ€™s pair of indices within the grid of thread blocks to determine the corresponding element in the matrices. Since the matrices are stored as one-dimensional arrays of length \texttt{DSIZE\_X * DSIZE\_Y}, each element is accessed using a flattened index \texttt{idx + width * idy}. For threads whose \texttt{x} and \texttt{y} indices fall within the matrix dimensions (i.e., less than the width and height, respectively), the kernel computes the sum of the elements from \texttt{A} and \texttt{B} at that position and stores the result in the corresponding element of \texttt{C}.

\lstinputlisting[caption={\ttfamily{add\_matrix.cu}}, style=code]{../../assignment2/add_matrix.cu}

\begin{lstlisting}[style=output]
[rcruzcan@g37n01 assignment2]$ nvcc add_matrix.cu -o objs/add_matrix
[rcruzcan@g37n01 assignment2]$ ./objs/add_matrix
A:
0.840188, 0.783099, 0.911647, 
0.335223, 0.277775, 0.477397, 
0.364784, 0.952230, 0.635712, 

B:
0.394383, 0.798440, 0.197551, 
0.768230, 0.553970, 0.628871, 
0.513401, 0.916195, 0.717297, 

==============

C = A + B:
1.234571, 1.581539, 1.109199, 
1.103452, 0.831745, 1.106268, 
0.878185, 1.868425, 1.353009, 
\end{lstlisting}

% ------------------------------------------------------------------------



\begin{exr}{
    \begin{itemize}
      \item Write a C++ function that multiplies two matrices of size $N\times N$.
      \item Write a CUDA kernel that multiplies two matrices of size $N\times N$.
      \item You should express this using a 1-dimensional vector:
        \begin{itemize}
          \item In a 1-d array the 2-d matrix will look like: 
          \begin{align*}
              [A_{11},\, A_{12},\, A_{13},\, \dots,\, A_{1N}, A_{21},\, A_{22},\, \dots,\, A_{2N}, \dots, A_{N1},\, A_{N2},\, \dots,\, A_{NN}]
          \end{align*}
        \end{itemize}
      \item In the kernel use 2-dimensions to express threads and blocks: \texttt{threadIdx.x}, \texttt{blockIdx.x} and \texttt{threadIdx.y}, \texttt{blockIdx.y}.
      \item Start from file \texttt{mult\_matrix.cu}.
      \item Add error checking:
        \begin{itemize}
          \item After allocating memory
          \item After copying from host to device
          \item After launching kernel
          \item After copying from device to host
        \end{itemize}
      \item Use a timer to measure how long the matrix multiplication takes on the CPU.
      \item Use a timer to measure how long the matrix multiplication takes on the GPU.
    \end{itemize}
}\end{exr}

Since the matrices in this exercise are also represented as flat arrays, the kernel implemented takes the same approach towards accessing the elements of the matrices that was used in the previous exercise. For two square matrices $A$ and $B$ of size $N$, the element $C_{ij}$ of $C = A\times B$ is given by:

\begin{align*}
    C_{ij} = \sum_{k=0}^N A_{ik}B_{kj}
\end{align*}

\noindent In the implemented kernel \texttt{matrix\_mul\_gpu}, each thread computes one element of the result matrix \texttt{C}. Specifically, the element at index \texttt{idy * size + idx} is assigned the value obtained by summing \texttt{A[idy * size + k] * B[k * size + idx]} over all \texttt{k} in a loop. This operation is performed by every thread whose \texttt{x} and \texttt{y} indices fall within the matrix dimensions.

The non-GPU version, \texttt{matrix\_mul\_cpu}, follows the same computational logic but performs the operation serially using three nested loops that iterate over the indices of the \texttt{C} array.

Both functions, \texttt{matrix\_mul\_gpu} and \texttt{matrix\_mul\_cpu}, were timed to compare their execution performance. The results for a representative run are shown in the output section below. In all tests, the GPU implementation was approximately 2.5 orders of magnitude faster than the CPU implementation.

\lstinputlisting[caption={\texttt{mult\_matrix.cu}}, style=code]{../../assignment2/mult_matrix.cu}

\begin{lstlisting}[style=output]
[rcruzcan@g37n01 assignment2]$ nvcc mult_matrix.cu -o objs/mult_matrix
[rcruzcan@g37n01 assignment2]$ ./objs/mult_matrix
A (256 x 256):
3.000000, 3.000000, 3.000000, 
3.000000, 3.000000, 3.000000, 
3.000000, 3.000000, 3.000000, 

B (256 x 256):
2.000000, 2.000000, 2.000000, 
2.000000, 2.000000, 2.000000, 
2.000000, 2.000000, 2.000000, 

==============

GPU:
Time for compute: 0.000318 seconds
GPU Result for C = A * B (256 x 256):
1536.000000, 1536.000000, 1536.000000, 
1536.000000, 1536.000000, 1536.000000, 
1536.000000, 1536.000000, 1536.000000, 

==============

CPU:
Time for compute: 0.099214 seconds
CPU Result for C = A * B (256 x 256):
1536.000000, 1536.000000, 1536.000000, 
1536.000000, 1536.000000, 1536.000000, 
1536.000000, 1536.000000, 1536.000000, 
\end{lstlisting}

\end{document}