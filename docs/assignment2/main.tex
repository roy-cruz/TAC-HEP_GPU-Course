\documentclass{article}
\usepackage{graphicx}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}
\usepackage{graphicx}
\usepackage{indentfirst}
\usepackage{tcolorbox}
\usepackage{xcolor}
\usepackage{amsmath}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\usepackage{listings}
\lstset{language=C++}
\lstset{breaklines=true}
\lstset{
    literate={"}{{\texttt{"}}}1
             {'}{{\texttt{'}}}1
             {~}{{\texttt{~}}}1
             { }{{\ }}1
}
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=t,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
\lstset{style=mystyle}

\lstdefinestyle{outputstyle}{
    backgroundcolor=\color{gray!10},
    basicstyle=\ttfamily\footnotesize,
    frame=single,
    breaklines=true,
    showstringspaces=false,
    numbers=none
}

\newcounter{exercise}
\newenvironment{exr}[1]{%
    \refstepcounter{exercise}
    \begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title=Exercise \theexercise]
    \textbf{Instructions:} #1
    \end{tcolorbox}
    \vspace{1em}
}{}

\patchcmd{\thebibliography}{\section*{\refname}}{}{}{}

\title{
    TAC-HEP GPU Programming Training Module: Assignment 2
}

\author{Roy F. Cruz}

\begin{document}

\date{October 12, 2025}
\maketitle

The GitHub repository hosting this code can be found through the following link: \href{https://github.com/roy-cruz/TAC-HEP_GPU-Course_Assignments/tree/master}{link}

\begin{exr}{
    \begin{itemize}
        \item Write a CUDA kernel that swaps the elements of two vectors.
        \item Start from file \texttt{swap\_vectors.cu}.
    \end{itemize}
    }
\end{exr}

The kernel in this implementation works by first using a temporary variable, \texttt{temp}, to store the element of \texttt{A} located at index \texttt{idx = threadIdx.x + blockDim.x * blockIdx.x}. It then replaces that element in \texttt{A} with the corresponding element from \texttt{B} at the same index. Finally, the value stored in \texttt{temp} is written back into \texttt{B} at that same position, effectively swapping the two elements. By ensuring that only threads with \texttt{idx < n\_elements} are executed, the kernel guarantees that each valid pair of elements between the two arrays is swapped exactly once and that no out-of-bounds memory access occurs.

\lstinputlisting[language=C++, caption={\ttfamily{swap\_vectors.cu}}]{../../assignment2/swap_vectors.cu}

% ------------------------------------------------------------------------


\begin{exr}{
    \begin{itemize}
      \item Write a CUDA kernel that adds two matrices of size $N\times M$.
      \item Express the indices of the matrices in 1 dimension:
        \begin{itemize}
          \item In a 1-d array the 2-d matrix will look like:
            \begin{align*}
            [A_{11},\, A_{12},\, A_{13},\, \dots,\, A_{1M}, A_{21},\, A_{22},\, \dots,\, A_{2M}, \dots, A_{N1},\, A_{N2},\, \dots,\, A_{NM}]
            \end{align*}
          \item For a matrix of size $N\times M$ each element can be expressed as \\
            \texttt{2D:[i,j] = 1D:[i * M + j]}
        \end{itemize}
      \item Use 2-dimensions to express threads and blocks: \texttt{threadIdx.x}, \texttt{blockIdx.x}, \texttt{threadIdx.y}, \texttt{blockIdx.y}.
      \item Start from file \texttt{add\_matrix.cu}.
    \end{itemize}   
}\end{exr}

The matrix addition kernel works by using each threadâ€™s pair of indices within the grid of thread blocks to determine the corresponding element in the matrices. Since the matrices are stored as one-dimensional arrays of length \texttt{DSIZE\_X * DSIZE\_Y}, each element is accessed using a flattened index \texttt{idx + width * idy}. For threads whose \texttt{x} and \texttt{y} indices fall within the matrix dimensions (i.e., less than the width and height, respectively), the kernel computes the sum of the elements from \texttt{A} and \texttt{B} at that position and stores the result in the corresponding element of \texttt{C}.

\lstinputlisting[language=C++, caption={\ttfamily{add\_matrix.cu}}]{../../assignment2/add_matrix.cu}



% ------------------------------------------------------------------------



\begin{exr}{
    \begin{itemize}
      \item Write a C++ function that multiplies two matrices of size $N\times N$.
      \item Write a CUDA kernel that multiplies two matrices of size $N\times N$.
      \item You should express this using a 1-dimensional vector:
        \begin{itemize}
          \item In a 1-d array the 2-d matrix will look like: 
          \begin{align*}
              [A_{11},\, A_{12},\, A_{13},\, \dots,\, A_{1N}, A_{21},\, A_{22},\, \dots,\, A_{2N}, \dots, A_{N1},\, A_{N2},\, \dots,\, A_{NN}]
          \end{align*}
        \end{itemize}
      \item In the kernel use 2-dimensions to express threads and blocks: \texttt{threadIdx.x}, \texttt{blockIdx.x} and \texttt{threadIdx.y}, \texttt{blockIdx.y}.
      \item Start from file \texttt{mult\_matrix.cu}.
      \item Add error checking:
        \begin{itemize}
          \item After allocating memory
          \item After copying from host to device
          \item After launching kernel
          \item After copying from device to host
        \end{itemize}
      \item Use a timer to measure how long the matrix multiplication takes on the CPU.
      \item Use a timer to measure how long the matrix multiplication takes on the GPU.
    \end{itemize}
}\end{exr}

Since the matrices in this exercise are also represented as flat arrays, the kernel implemented takes the same approach towards accessing the elements of the matrices that was used in the previous exercise. For two square matrices $A$ and $B$ of size $N$, the element $C_{ij}$ of $C = A\times B$ is given by:

\begin{align*}
    C_{ij} = \sum_{k=0}^N A_{ik}B_{kj}
\end{align*}

\noindent Thus, in the implemented kernel \texttt{matrix\_mul\_gpu}, the element with index \texttt{idy * size + idx} of the result matrix \texttt{C} is assigned the value of summing \texttt{A[idy * size + k] * B[k * size + idx]} along \texttt{k} in a loop. Each thread for which their indices in \texttt{x} and \texttt{y} fall inside the matrix dimensions perform this operation.

The non-GPU implementation of this, \texttt{matrix\_mul\_cpu}, takes a similar approach. However, there are two additional loops that are performed that serially iterate over both indices of the \texttt{C} array.

The calls to \texttt{matrix\_mul\_gpu} and \texttt{matrix\_mul\_cpu} were timed, and it was found that the matrix 

\lstinputlisting[language=C++, caption={\texttt{mult\_matrix.cu}}]{../../assignment2/mult_matrix.cu}

\end{document}